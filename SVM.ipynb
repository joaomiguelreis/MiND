{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some experiments with SVM. Here I test the 3 main SVC (Support Vector Classification) algorithms: Linear SVM, Nu-SVM and C-SVM. The N-SVM seems to be the best but we need more experiments.\n",
    "\n",
    "This analysis is very simple. I shuffle my dataset and take $30\\%$ of testing. I do this $M$ times and store each shuffled data set. Then I preform $M$ classifications and take the mean of thhe numer of miss-classified samples and corresponding variance.\n",
    "\n",
    "This is only for a single cell (like Anabel did). I guess we should have to repeat this for all cells in the mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, Math #\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup pandas options\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "\n",
    "# Setup numpy options\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# Get the data\n",
    "PT_data = pd.read_excel(\"PTResults_trimmed.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can change the number of shuffles and test size. This will store the training and testing features and targets to be used for any algorithms you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Temperture[K]  AoA[o]  Mach  MVD[mum]  rho[kg/m3]\n",
      "count           70.0    70.0  70.0      70.0        70.0\n",
      "mean           250.1     5.7   0.6       0.0         0.8\n",
      "std             11.2     3.2   0.1       0.0         0.3\n",
      "min            233.2     0.1   0.3       0.0         0.4\n",
      "25%            240.3     3.2   0.4       0.0         0.6\n",
      "50%            249.5     6.3   0.6       0.0         0.8\n",
      "75%            258.0     8.0   0.7       0.0         1.0\n",
      "max            272.7     9.9   0.8       0.0         1.2\n"
     ]
    }
   ],
   "source": [
    "## TRAINING SVM  ##\n",
    "\n",
    "target = 0.617035028915481\n",
    "X = PT_data.drop(target, axis=1) # Just preparing the data\n",
    "y = PT_data[target]\n",
    "\n",
    "training_features_vector = []\n",
    "testing_features_vector = []\n",
    "training_target_vector = []\n",
    "testing_target_vector = []\n",
    "\n",
    "M = 1000 # number of shuffles\n",
    "test_size = 30 # number of cells to be tested out of 100\n",
    "\n",
    "#Splitting the data into training and testing and storing to reuze same seeds\n",
    "# for the different algorithms\n",
    "for i in range(M):\n",
    "    \n",
    "    training_features, testing_features, training_target, testing_target = train_test_split(\n",
    "        X, y, test_size=test_size/100, shuffle=True)\n",
    "    #store\n",
    "    training_features_vector.append(training_features)\n",
    "    testing_features_vector.append(testing_features)\n",
    "    training_target_vector.append(training_target)\n",
    "    testing_target_vector.append(testing_target)\n",
    "    \n",
    "print(training_features.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the kernel $K(x,x') := \\langle x, x'\\rangle$. It's here just for funny. It gives very bad results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average number of miss classified cells out of 1000 shuffles: 7.364 +- 0.536119586253\n"
     ]
    }
   ],
   "source": [
    "# Linear Support Vector Classification\n",
    "MSE_linear = 0\n",
    "VarMSE_linear = 0\n",
    "missclass = []\n",
    "for i in range(M):\n",
    "    training_features = training_features_vector[i]\n",
    "    testing_features = testing_features_vector[i]\n",
    "    training_target = training_target_vector[i]\n",
    "    testing_target = testing_target_vector[i]\n",
    "\n",
    "    #Training with Linear SVC\n",
    "    model = LinearSVC(random_state=0, tol=1e-5)\n",
    "    model.fit(training_features, training_target)\n",
    "\n",
    "    #Comparing prediction with testing values\n",
    "    prediction = model.predict(testing_features)\n",
    "    \n",
    "    #Get means and std\n",
    "    MSE = mean_squared_error(prediction, testing_target)\n",
    "    MSE_linear += MSE\n",
    "    VarMSE_linear += MSE*MSE\n",
    "    missclass.append(30*mean_squared_error(prediction, testing_target))\n",
    "    \n",
    "MSE_linear /= M\n",
    "VarMSE_linear -= M*MSE_linear**2\n",
    "VarMSE_linear /=(M-1)\n",
    "count_linear = test_size*MSE_linear\n",
    "countstd_linear = test_size*VarMSE_linear\n",
    "print('Average number of miss classified cells out of ' + str(M) + ' shuffles: ' + \n",
    "      str(count_linear) + ' +- ' + str(countstd_linear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nu-Support Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See documentation: https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html\n",
    "\n",
    "We start by choosing the kernel: 'rbf' or 'poly'. \n",
    "    - If 'rbf' is choosen then we can choose two parameters: $\\nu$ and $\\gamma$\n",
    "    - If 'poly' or 'sigmoid' is choosen then we can choose three parameters: $\\nu$, $\\gamma$ and polynomial degree\n",
    "\n",
    "The values of the parameters to be tested are in: nu_vector, gamma_vector and degree_vector. You may change these values if you want. Be carefull, for dregree > 3 things become slow for $\\gamma > 0.01$.\n",
    "\n",
    "The strategy to find to optimum is to save all combinations of parameters that give $<15\\%$ misclassification with variance $<5\\%$. To do so we start with kernel = 'rbf' to tune the gamma values. We will see that the kernel = 'poly' is the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier function\n",
    "\n",
    "This is our personalised classifier that returns the mean squared error, the variance of the MSE and the number of misclassified shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nu-Support Vector Classification\n",
    "def mySVC_Nu(nu, kernel, gamma, degree=0):\n",
    "\n",
    "    MSE_Nu = 0\n",
    "    VarMSE_Nu = 0\n",
    "    missclass = []\n",
    "    for i in range(M):\n",
    "        \n",
    "        training_features = training_features_vector[i]\n",
    "        testing_features = testing_features_vector[i]\n",
    "        training_target = training_target_vector[i]\n",
    "        testing_target = testing_target_vector[i]\n",
    "\n",
    "        #Training with Nu-SVC\n",
    "        model = NuSVC(nu = nu, kernel = kernel, degree=degree, gamma=gamma)\n",
    "            \n",
    "        model.fit(training_features, training_target)\n",
    "\n",
    "        #Comparing prediction with testing values\n",
    "        prediction = model.predict(testing_features)\n",
    "\n",
    "        #Get means and std\n",
    "        MSE = mean_squared_error(prediction, testing_target)\n",
    "        MSE_Nu += MSE\n",
    "        VarMSE_Nu += MSE*MSE\n",
    "        missclass.append(30*mean_squared_error(prediction, testing_target))\n",
    "\n",
    "    MSE_Nu /= M\n",
    "    VarMSE_Nu -= M*MSE_Nu**2\n",
    "    VarMSE_Nu /=(M-1)\n",
    "#     count_Nu = test_size*MSE_Nu\n",
    "#     countVar_Nu = test_size*VarMSE_Nu\n",
    "           \n",
    "    return MSE_Nu, VarMSE_Nu, missclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel = 'rbf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# simulations done 10/28\n",
      "# simulations done 20/28\n",
      "candidate indices for optimal distributions\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "nu_vector = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "gamma_vector = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "\n",
    "\n",
    "# kernel = 'rbf'\n",
    "opt_rbf = []\n",
    "n_simul = len(nu_vector)*len(gamma_vector)\n",
    "count = 0\n",
    "for i in range(len(nu_vector)):\n",
    "    nu = nu_vector[i]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print('# simulations done ' + str(count) + '/' + str(n_simul))\n",
    "        gamma = gamma_vector[j]\n",
    "        MSE_Nu, VarMSE_Nu, missclass = mySVC_Nu(nu=nu, kernel = 'rbf', gamma=gamma, degree=0)\n",
    "        if test_size*MSE_Nu/test_size < 0.15:\n",
    "            opt_rbf.append([i,j])\n",
    "\n",
    "print('candidate indices for optimal distributions')                \n",
    "print(opt_rbf)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of simulations to be done 135\n",
      "# simulations done 20/135\n",
      "# simulations done 40/135\n",
      "# simulations done 60/135\n",
      "# simulations done 80/135\n",
      "# simulations done 100/135\n",
      "# simulations done 120/135\n",
      "candidate indices for optimal distributions\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "nu_vector = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6]\n",
    "gamma_vector = [0.00001, 0.0001, 0.001]\n",
    "degree_vector = [0,1,2,3,4]\n",
    "\n",
    "# counting how many simulations to be done\n",
    "n_simul_sigm = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        for i in range(len(nu_vector)):\n",
    "            n_simul_sigm += 1\n",
    "\n",
    "print('number of simulations to be done ' + str(n_simul_sigm))\n",
    "\n",
    "\n",
    "# kernel = 'poly'\n",
    "opt_sigm = []\n",
    "count = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        for i in range(len(nu_vector)):\n",
    "            count +=1\n",
    "            if count % 20 == 0:\n",
    "                print('# simulations done ' + str(count) + '/' + str(n_simul_sigm))\n",
    "            nu = nu_vector[i]\n",
    "            MSE_Nu, VarMSE_Nu, missclass = mySVC_Nu(nu=nu, kernel = 'sigmoid', gamma=gamma, degree=degree)\n",
    "            if test_size*MSE_Nu/test_size < 0.15:\n",
    "                opt_sigm.append([i,j,k])\n",
    "\n",
    "print('candidate indices for optimal distributions')                \n",
    "print(opt_sigm)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel = 'poly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of simulations to be done 117\n",
      "# simulations done 20/117\n",
      "# simulations done 40/117\n",
      "# simulations done 60/117\n",
      "# simulations done 80/117\n",
      "# simulations done 100/117\n",
      "\n",
      "candidate indices for optimal distributions\n",
      "[[3, 1, 3], [4, 1, 3], [2, 1, 4], [3, 1, 4], [4, 1, 4]]\n"
     ]
    }
   ],
   "source": [
    "nu_vector = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6]\n",
    "gamma_vector = [0.00001, 0.0001, 0.001]\n",
    "degree_vector = [0,1,2,3,4]\n",
    "\n",
    "# counting how many simulations to be done\n",
    "n_simul_poly = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        if degree >= 2 and gamma >= 0.01: continue\n",
    "        if degree >= 3 and gamma >= 0.001: continue\n",
    "        for i in range(len(nu_vector)):\n",
    "            n_simul_poly += 1\n",
    "\n",
    "print('number of simulations to be done ' + str(n_simul_poly))\n",
    "\n",
    "\n",
    "# kernel = 'poly'\n",
    "opt_poly = []\n",
    "count = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        if degree >= 2 and gamma >= 0.01: continue\n",
    "        if degree >= 3 and gamma >= 0.001: continue\n",
    "        for i in range(len(nu_vector)):\n",
    "            count +=1\n",
    "            if count % 20 == 0:\n",
    "                print('# simulations done ' + str(count) + '/' + str(n_simul_poly))\n",
    "            nu = nu_vector[i]\n",
    "            MSE_Nu, VarMSE_Nu, missclass = mySVC_Nu(nu=nu, kernel = 'poly', gamma=gamma, degree=degree)\n",
    "            if test_size*MSE_Nu/test_size < 0.15:\n",
    "                opt_poly.append([i,j,k])\n",
    "\n",
    "\n",
    "                \n",
    "print()                \n",
    "print('candidate indices for optimal distributions')                \n",
    "print(opt_poly)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate once again the candidates for optimum to get the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average # misclassified samples from 30 samples: 4.366 +- 0.148416549883 with gamma = 0.0001, degree 3 and nu = 0.25\n",
      "Average # misclassified samples from 30 samples: 4.359 +- 0.149553520187 with gamma = 0.0001, degree 3 and nu = 0.3\n",
      "Average # misclassified samples from 30 samples: 4.294 +- 0.166552018685 with gamma = 0.0001, degree 4 and nu = 0.2\n",
      "Average # misclassified samples from 30 samples: 3.94 +- 0.111791791792 with gamma = 0.0001, degree 4 and nu = 0.25\n",
      "Average # misclassified samples from 30 samples: 4.081 +- 0.148029329329 with gamma = 0.0001, degree 4 and nu = 0.3\n"
     ]
    }
   ],
   "source": [
    "axes_vector = []\n",
    "for i in range(len(opt_poly)):\n",
    "    nu = nu_vector[opt_poly[i][0]]\n",
    "    gamma = gamma_vector[opt_poly[i][1]]\n",
    "    degree = degree_vector[opt_poly[i][2]]\n",
    "    MSE_Nu, VarMSE_Nu, missclass = mySVC_Nu(nu, kernel = 'poly', gamma=gamma, degree=degree)\n",
    "    axes_vector.append(missclass)\n",
    "    print(r'Average # misclassified samples from ' + str(test_size) + r' samples: ' + \n",
    "              str(test_size*MSE_Nu) + r' +- ' + str(test_size*VarMSE_Nu) + r' with gamma = ' + str(gamma) + \n",
    "         r', degree ' + str(degree) + ' and nu = ' + str(nu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABIkAAADPCAYAAABx/IE6AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAQDSJAAAAAIgmEQAAAABJ/j8Vt+JIGlk+jAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x116069860>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plotting for optimum\n",
    "rows = 1\n",
    "columns = len(axes_vector)\n",
    "fig, ax = plt.subplots(rows, columns, figsize=(20,3), sharex='col', sharey='row')\n",
    "plt.rcParams.update({'font.size': 8})\n",
    "\n",
    "\n",
    "for j in range(columns):\n",
    "    ax[j].hist(axes_vector[j], bins=10)\n",
    "    ax[j].set_title('degree = ' + str(degree_vector[opt_poly[j][2]]) + ', gamma = ' + str (gamma_vector[opt_poly[j][1]]) \\\n",
    "                    + ' and nu = ' + str(nu_vector[opt_poly[j][0]]))\n",
    "    ax[j].set_ylim(0,500)\n",
    "    ax[j].set_xlim(0,15)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C-Support Vector Classification\n",
    "\n",
    "see documentation: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "\n",
    "Same as for Nu-SVC (very little changes). Parameter $\\nu$ becomes the parameter C.\n",
    "\n",
    "YOU CAN USE THE PARAMETERS YOU WANT BUT IT IS VERY UNLIKELY THERE WILL BE A CANDIDATE FOR OPTIMAL DISTRIBUTION. The mean may get low but variance is too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-Support Vector Classification\n",
    "def mySVC(C, kernel, gamma, degree):  # changed the name of the function\n",
    "\n",
    "    MSE_svc = 0\n",
    "    VarMSE_svc = 0\n",
    "    missclass = []\n",
    "    for i in range(M):\n",
    "        training_features = training_features_vector[i]\n",
    "        testing_features = testing_features_vector[i]\n",
    "        training_target = training_target_vector[i]\n",
    "        testing_target = testing_target_vector[i]\n",
    "\n",
    "        #Training with Nu-SVC\n",
    "        model = SVC(C=C, kernel=kernel, degree=degree, gamma=gamma)\n",
    "        model.fit(training_features, training_target)\n",
    "\n",
    "        #Comparing prediction with testing values\n",
    "        prediction = model.predict(testing_features)\n",
    "\n",
    "        #Get means and std\n",
    "        MSE = mean_squared_error(prediction, testing_target)\n",
    "        MSE_svc += MSE\n",
    "        VarMSE_svc += MSE*MSE\n",
    "        missclass.append(30*mean_squared_error(prediction, testing_target))\n",
    "\n",
    "    MSE_svc /= M\n",
    "    VarMSE_svc -= M*MSE_svc**2\n",
    "    VarMSE_svc/=(M-1)\n",
    "    count_svc = test_size*MSE_svc\n",
    "    countVar_svc = test_size*VarMSE_svc\n",
    "    \n",
    "           \n",
    "    return MSE, VarMSE, missclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel = 'rbf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# simulations done 10/28\n",
      "# simulations done 20/28\n",
      "no condidates for optimal distributions\n"
     ]
    }
   ],
   "source": [
    "C_vector = [0.2, 0.4, 0.8, 1, 2, 5, 10]\n",
    "gamma_vector = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "# kernel = 'rbf'\n",
    "opt_rbf = []\n",
    "n_simul = len(C_vector)*len(gamma_vector)\n",
    "count = 0\n",
    "for i in range(len(C_vector)):\n",
    "    C = C_vector[i]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print('# simulations done ' + str(count) + '/' + str(n_simul))\n",
    "        gamma = gamma_vector[j]\n",
    "        MSE, VarMSE, missclass = mySVC(C=C, kernel = 'rbf', gamma=gamma, degree=0) # changed mySVC\n",
    "        if test_size*MSE/test_size < 0.15 and test_size*VarMSE/test_size < 0.05:\n",
    "            opt_rbf.append([i,j])\n",
    "\n",
    "print()\n",
    "if len(opt_rbf) == 0: print('no condidates for optimal distributions')\n",
    "else:        \n",
    "    print('candidate indices for optimal distributions')\n",
    "    print(opt_rbf)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_vector = []\n",
    "for i in range(len(opt_rbf)):\n",
    "    C = C_vector[opt_rbf[i][0]]\n",
    "    gamma = gamma_vector[opt_rbf[i][1]]\n",
    "    MSE, VarMSE, missclass = mySVC(C, kernel = 'rbf', gamma=gamma, degree=0)\n",
    "    axes_vector.append(missclass)\n",
    "    print(r'Average # misclassified samples from ' + str(test_size) + r' samples: ' + \n",
    "              str(test_size*MSE) + r' +- ' + str(test_size*VarMSE) + r' with gamma = ' + str(gamma) + \n",
    "         r', degree ' + str(degree) + ' and C = ' + str(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of simulations to be done 135\n",
      "# simulations done 20/135\n",
      "# simulations done 40/135\n",
      "# simulations done 60/135\n",
      "# simulations done 80/135\n",
      "# simulations done 100/135\n",
      "# simulations done 120/135\n",
      "candidate indices for optimal distributions\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "C_vector = [0.2, 0.4, 0.8, 1, 2, 5, 10]\n",
    "gamma_vector = [0.00001, 0.0001, 0.001]\n",
    "degree_vector = [0,1,2,3,4]\n",
    "\n",
    "# counting how many simulations to be done\n",
    "n_simul_sigm = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        for i in range(len(nu_vector)):\n",
    "            n_simul_sigm += 1\n",
    "\n",
    "print('number of simulations to be done ' + str(n_simul_sigm))\n",
    "\n",
    "\n",
    "# kernel = 'sigmoid'\n",
    "opt_sigm = []\n",
    "count = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        for i in range(len(nu_vector)):\n",
    "            count +=1\n",
    "            if count % 20 == 0:\n",
    "                print('# simulations done ' + str(count) + '/' + str(n_simul_sigm))\n",
    "            nu = nu_vector[i]\n",
    "            MSE, VarMSE, missclass = mySVC(C=C, kernel = 'sigmoid', gamma=gamma, degree=degree)\n",
    "            if test_size*MSE/test_size < 0.15 and test_size*VarMSE/test_size < 0.05:\n",
    "                opt_sigm.append([i,j,k])\n",
    "\n",
    "print()\n",
    "if len(opt_sigm == 0: print('no condidates for optimal distributions')\n",
    "else:        \n",
    "    print('candidate indices for optimal distributions')\n",
    "    print(opt_sigm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel = 'poly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of simulations to be done 117\n",
      "# simulations done 20/117\n",
      "# simulations done 40/117\n",
      "# simulations done 60/117\n",
      "# simulations done 80/117\n",
      "# simulations done 100/117\n",
      "\n",
      "candidate indices for optimal distributions\n",
      "[]\n"
     ]
    }
   ],
   "source": [
    "C_vector = [0.2, 0.4, 0.8, 1, 2, 5, 10]\n",
    "gamma_vector = [0.00001, 0.0001, 0.001]\n",
    "degree_vector = [0,1,2,3,4]\n",
    "\n",
    "# counting how many simulations to be done\n",
    "n_simul_poly = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        if degree >= 2 and gamma >= 0.01: continue\n",
    "        if degree >= 3 and gamma >= 0.001: continue\n",
    "        for i in range(len(nu_vector)):\n",
    "            n_simul_poly += 1\n",
    "\n",
    "print('number of simulations to be done ' + str(n_simul_poly))\n",
    "\n",
    "\n",
    "# kernel = 'poly'\n",
    "opt_poly = []\n",
    "count = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        if degree >= 2 and gamma >= 0.01: continue\n",
    "        if degree >= 3 and gamma >= 0.001: continue\n",
    "        for i in range(len(nu_vector)):\n",
    "            count +=1\n",
    "            if count % 20 == 0:\n",
    "                print('# simulations done ' + str(count) + '/' + str(n_simul_poly))\n",
    "            nu = nu_vector[i]\n",
    "            MSE, VarMSE, missclass = mySVC(C=C, kernel = 'poly', gamma=gamma, degree=degree)\n",
    "            if test_size*MSE/test_size < 0.15 and test_size*VarMSE/test_size < 0.05:\n",
    "                opt_poly.append([i,j,k])\n",
    "\n",
    "\n",
    "print()              \n",
    "if len(opt_rbf) == 0: print('no condidates for optimal distributions')\n",
    "else:        \n",
    "    print('candidate indices for optimal distributions')\n",
    "    print(opt_rbf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Simulate once again the candidates for optimum to get the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_vector = []\n",
    "for i in range(len(opt_poly)):\n",
    "    nu = nu_vector[opt_poly[i][0]]\n",
    "    gamma = gamma_vector[opt_poly[i][1]]\n",
    "    degree = degree_vector[opt_poly[i][2]]\n",
    "    MSE, VarMSE, missclass = mySVC(C=C, kernel = 'poly', gamma=gamma, degree=degree)\n",
    "    axes_vector.append(missclass)\n",
    "    print(r'Average # misclassified samples from ' + str(test_size) + r' samples: ' + \n",
    "              str(test_size*MSE) + r' +- ' + str(test_size*VarMSE) + r' with gamma = ' + str(gamma) + \n",
    "         r', degree ' + str(degree) + ' and C = ' + str(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plotting histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
