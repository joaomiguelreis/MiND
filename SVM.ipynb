{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are some experiments with SVM. Here I test the 3 main SVC (Support Vector Classification) algorithms: Linear SVM, Nu-SVM and C-SVM. The N-SVM seems to be the best but we need more experiments.\n",
    "\n",
    "This analysis is very simple. I shuffle my dataset and take $30\\%$ of testing. I do this $M$ times and store each shuffled data set. Then I preform $M$ classifications and take the mean of thhe numer of miss-classified samples and corresponding variance.\n",
    "\n",
    "This is only for a single cell (like Anabel did). I guess we should have to repeat this for all cells in the mesh."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from IPython.display import display, Math #\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from sklearn.svm import LinearSVC, NuSVC, SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup pandas options\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "\n",
    "# Setup numpy options\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# Get the data\n",
    "# PT_data = pd.read_excel(\"PTResults_trimmed.xlsx\")\n",
    "\n",
    "\n",
    "# PT_data = pd.read_excel(\"PTResults.xlsx\")\n",
    "# M = 100 # number of shuffles\n",
    "# test_size = 15 # number of cells to be tested out of 100\n",
    "\n",
    "\n",
    "\n",
    "PT_data = pd.read_excel(\"PTResults_ice.xlsx\")\n",
    "M = 1000 # number of shuffles\n",
    "test_size = 150 # number of cells to be tested out of 100\n",
    "# print(PT_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can change the number of shuffles and test size. This will store the training and testing features and targets to be used for any algorithms you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## TRAINING SVM  ##\n",
    "\n",
    "data_matrix = np.asarray(PT_data)\n",
    "X_aux = data_matrix[:,0:5]\n",
    "X = []\n",
    "y = []\n",
    "n_loc = len(data_matrix[0,5:-1])\n",
    "for i in range(5):\n",
    "    for loc in range(7,n_loc):\n",
    "        x = np.append(X_aux[i,:], loc-5)\n",
    "        X.append(x)\n",
    "        y.append(data_matrix[i,loc-5].astype(int))\n",
    "\n",
    "training_features_vector = []\n",
    "testing_features_vector = []\n",
    "training_target_vector = []\n",
    "testing_target_vector = []\n",
    "\n",
    "\n",
    "\n",
    "#Splitting the data into training and testing and storing to reuze same seeds\n",
    "# for the different algorithms\n",
    "for i in range(M):\n",
    "    \n",
    "    #Splitting the data into training and testing\n",
    "    training_features, testing_features, training_target, testing_target = train_test_split(\n",
    "        X, y, test_size=test_size/M, shuffle=True)\n",
    "\n",
    "    #store\n",
    "    training_features_vector.append(training_features)\n",
    "    testing_features_vector.append(testing_features)\n",
    "    training_target_vector.append(training_target)\n",
    "    testing_target_vector.append(testing_target)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This uses the kernel $K(x,x') := \\langle x, x'\\rangle$. It's here just for fun. It gives very bad results..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum % of miss classified cells out of 1000 shuffles and 1000 samples : 0.758620689655\n"
     ]
    }
   ],
   "source": [
    "# Linear Support Vector Classification\n",
    "MSE_linear = 0\n",
    "VarMSE_linear = 0\n",
    "missclass = []\n",
    "MSE_old = 1\n",
    "for i in range(M):\n",
    "    training_features = training_features_vector[i]\n",
    "    testing_features = testing_features_vector[i]\n",
    "    training_target = training_target_vector[i]\n",
    "    testing_target = testing_target_vector[i]\n",
    "    #Training with Linear SVC\n",
    "    model = LinearSVC(random_state=0, tol=1e-5)\n",
    "    model.fit(training_features, training_target)\n",
    "\n",
    "    #Comparing prediction with testing values\n",
    "    prediction = model.predict(testing_features)\n",
    "    \n",
    "    #Get means and std\n",
    "    MSE = mean_squared_error(prediction, testing_target)\n",
    "    if MSE < MSE_old:\n",
    "        model_old = model\n",
    "#     MSE_linear += MSE\n",
    "#     VarMSE_linear += MSE*MSE\n",
    "    missclass.append(test_size*mean_squared_error(prediction, testing_target))\n",
    "    \n",
    "# MSE_linear /= M\n",
    "# VarMSE_linear -= M*MSE_linear**2\n",
    "# VarMSE_linear /=(M-1)\n",
    "# count_linear = test_size*MSE_linear\n",
    "# countstd_linear = test_size*VarMSE_linear\n",
    "print('Minimum % of miss classified cells out of 1000 shuffles and ' + \\\n",
    "      str(M) + ' samples : ' + str(MSE))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nu-Support Vector Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See documentation: https://scikit-learn.org/stable/modules/generated/sklearn.svm.NuSVC.html\n",
    "\n",
    "We start by choosing the kernel: 'rbf' or 'poly'. \n",
    "    - If 'rbf' is choosen then we can choose two parameters: $\\nu$ and $\\gamma$\n",
    "    - If 'poly' or 'sigmoid' is choosen then we can choose three parameters: $\\nu$, $\\gamma$ and polynomial degree\n",
    "\n",
    "The values of the parameters to be tested are in: nu_vector, gamma_vector and degree_vector. You may change these values if you want. Be carefull, for dregree > 3 things become slow for $\\gamma > 0.01$.\n",
    "\n",
    "The strategy to find to optimum is to save all combinations of parameters that give $<15\\%$ misclassification with variance $<5\\%$. To do so we start with kernel = 'rbf' to tune the gamma values. We will see that the kernel = 'poly' is the best. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classifier function\n",
    "\n",
    "This is our personalised classifier that returns the mean squared error, the variance of the MSE and the number of misclassified shuffles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nu-Support Vector Classification\n",
    "def mySVC_Nu(nu, kernel, gamma, degree=0):\n",
    "\n",
    "    MSE_old = 1\n",
    "    for i in range(M):\n",
    "        \n",
    "        training_features = training_features_vector[i]\n",
    "        testing_features = testing_features_vector[i]\n",
    "        training_target = training_target_vector[i]\n",
    "        testing_target = testing_target_vector[i]\n",
    "\n",
    "        #Training with Nu-SVC\n",
    "        model = NuSVC(nu = nu, kernel = kernel, degree=degree, gamma=gamma)  \n",
    "        model.fit(training_features, training_target)\n",
    "\n",
    "        #Comparing prediction with testing values\n",
    "        prediction = model.predict(testing_features)\n",
    "\n",
    "        #Get means and std\n",
    "        MSE = mean_squared_error(prediction, testing_target)\n",
    "        if MSE < MSE_old:\n",
    "            MSE_old  = MSE\n",
    "#         missclass.append(test_size*mean_squared_error(prediction, testing_target))\n",
    "           \n",
    "    return MSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel = 'rbf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nu_vector = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4]\n",
    "gamma_vector = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "\n",
    "\n",
    "# kernel = 'rbf'\n",
    "opt_rbf = []\n",
    "n_simul = len(nu_vector)*len(gamma_vector)\n",
    "count = 0\n",
    "for i in range(len(nu_vector)):\n",
    "    nu = nu_vector[i]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print('# simulations done ' + str(count) + '/' + str(n_simul))\n",
    "        gamma = gamma_vector[j]\n",
    "        MSE = mySVC_Nu(nu=nu, kernel = 'rbf', gamma=gamma, degree=0)\n",
    "        if MSE < 0.001:\n",
    "            opt_rbf.append([i,j])\n",
    "\n",
    "print('candidate indices for optimal distributions')                \n",
    "print(opt_rbf)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_vector = []\n",
    "for i in range(len(opt_rbf)):\n",
    "    nu = nu_vector[opt_rbf[i][0]]\n",
    "    gamma = gamma_vector[opt_rbf[i][1]]\n",
    "#     degree = degree_vector[opt_rbf[i][2]]\n",
    "    MSE, VarMSE, missclass = mySVC_Nu(nu=nu, kernel = 'sigmoid', gamma=gamma, degree=0)\n",
    "    axes_vector.append(missclass)\n",
    "    print('Average % of miss classified cells out of 1000 shuffles and ' + \\\n",
    "          str(M) + ' samples : ' + str(MSE_linear) + ' +- ' + str(VarMSE_linear))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nu_vector = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6]\n",
    "gamma_vector = [0.00001, 0.0001, 0.001]\n",
    "degree_vector = [2,3,4]\n",
    "\n",
    "# counting how many simulations to be done\n",
    "n_simul_sigm = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        for i in range(len(nu_vector)):\n",
    "            n_simul_sigm += 1\n",
    "\n",
    "print('number of simulations to be done ' + str(n_simul_sigm))\n",
    "\n",
    "\n",
    "# kernel = 'poly'\n",
    "opt_sigm = []\n",
    "count = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        for i in range(len(nu_vector)):\n",
    "            count +=1\n",
    "            if count % 20 == 0:\n",
    "                print('# simulations done ' + str(count) + '/' + str(n_simul_sigm))\n",
    "            nu = nu_vector[i]\n",
    "            MSE, VarMSE, missclass = mySVC_Nu(nu=nu, kernel = 'sigmoid', gamma=gamma, degree=degree)\n",
    "            if test_size*MSE/test_size < 0.06 and test_size*VarMSE/test_size < 0.005:\n",
    "                opt_sigm.append([i,j,k])\n",
    "\n",
    "print('candidate indices for optimal distributions')                \n",
    "print(opt_sigm)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_vector = []\n",
    "for i in range(len(opt_sigm)):\n",
    "    nu = nu_vector[opt_sigm[i][0]]\n",
    "    gamma = gamma_vector[opt_sigm[i][1]]\n",
    "    degree = degree_vector[opt_sigm[i][2]]\n",
    "    MSE, VarMSE, missclass = mySVC_Nu(nu, kernel = 'sigmoid', gamma=gamma, degree=degree)\n",
    "    axes_vector.append(missclass)\n",
    "    print(r'Average # misclassified samples from ' + str(test_size) + r' samples: ' + \n",
    "              str(test_size*MSE) + r' +- ' + str(test_size*VarMSE) + r' with gamma = ' + str(gamma) + \n",
    "         r', degree ' + str(degree) + ' and nu = ' + str(nu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel = 'poly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nu_vector = [0.1, 0.15, 0.2, 0.25, 0.3, 0.35]\n",
    "gamma_vector = [0.00001, 0.0001, 0.001]\n",
    "degree_vector = [2,3,4]\n",
    "\n",
    "# counting how many simulations to be done\n",
    "n_simul_poly = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        if degree >= 2 and gamma >= 0.01: continue\n",
    "        if degree >= 3 and gamma >= 0.001: continue\n",
    "        for i in range(len(nu_vector)):\n",
    "            n_simul_poly += 1\n",
    "\n",
    "print('number of simulations to be done ' + str(n_simul_poly))\n",
    "\n",
    "\n",
    "# kernel = 'poly'\n",
    "opt_poly = []\n",
    "count = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        if degree >= 2 and gamma >= 0.01: continue\n",
    "        if degree >= 3 and gamma >= 0.001: continue\n",
    "        for i in range(len(nu_vector)):\n",
    "            count +=1\n",
    "            if count % 20 == 0:\n",
    "                print('# simulations done ' + str(count) + '/' + str(n_simul_poly))\n",
    "            nu = nu_vector[i]\n",
    "            MSE, VarMSE, missclass = mySVC_Nu(nu=nu, kernel = 'poly', gamma=gamma, degree=degree)\n",
    "            if test_size*MSE/test_size < 0.06 and test_size*VarMSE/test_size < 0.005:\n",
    "                opt_poly.append([i,j,k])\n",
    "\n",
    "\n",
    "                \n",
    "print()                \n",
    "print('candidate indices for optimal distributions')                \n",
    "print(opt_poly)            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Simulate once again the candidates for optimum to get the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_vector = []\n",
    "for i in range(len(opt_poly)):\n",
    "    nu = nu_vector[opt_poly[i][0]]\n",
    "    gamma = gamma_vector[opt_poly[i][1]]\n",
    "    degree = degree_vector[opt_poly[i][2]]\n",
    "    MSE, VarMSE, missclass = mySVC_Nu(nu, kernel = 'poly', gamma=gamma, degree=degree)\n",
    "    axes_vector.append(missclass)\n",
    "    print(r'Average # misclassified samples from ' + str(test_size) + r' samples: ' + \n",
    "              str(test_size*MSE) + r' +- ' + str(test_size*VarMSE) + r' with gamma = ' + str(gamma) + \n",
    "         r', degree ' + str(degree) + ' and nu = ' + str(nu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Plotting histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting for optimum\n",
    "rows = 3\n",
    "columns = 3\n",
    "fig, ax = plt.subplots(rows, columns, figsize=(20,10), sharex='col', sharey='row')\n",
    "plt.rcParams.update({'font.size': 9})\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(columns):\n",
    "        ax[i,j].hist(axes_vector[columns*i + j], bins=10)\n",
    "        ax[i,j].set_title('degree = ' + str(degree_vector[opt_poly[columns*i + j][2]]) + ', gamma = ' + str (gamma_vector[opt_poly[j][1]]) \\\n",
    "                        + ' and nu = ' + str(nu_vector[opt_poly[columns*i + j][0]]))\n",
    "        ax[i,j].set_ylim(0,500)\n",
    "        ax[i,j].set_xlim(0,2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C-Support Vector Classification\n",
    "\n",
    "see documentation: https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html#sklearn.svm.SVC\n",
    "\n",
    "Same as for Nu-SVC (very little changes). Parameter $\\nu$ becomes the parameter C.\n",
    "\n",
    "YOU CAN USE THE PARAMETERS YOU WANT BUT IT IS VERY UNLIKELY THERE WILL BE A CANDIDATE FOR OPTIMAL DISTRIBUTION. The mean may get low but variance is too large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# C-Support Vector Classification\n",
    "def mySVC(C, kernel, gamma, degree):  # changed the name of the function\n",
    "\n",
    "    MSE_svc = 0\n",
    "    VarMSE_svc = 0\n",
    "    missclass = []\n",
    "    for i in range(M):\n",
    "        training_features = training_features_vector[i]\n",
    "        testing_features = testing_features_vector[i]\n",
    "        training_target = training_target_vector[i]\n",
    "        testing_target = testing_target_vector[i]\n",
    "\n",
    "        #Training with Nu-SVC\n",
    "        model = SVC(C=C, kernel=kernel, degree=degree, gamma=gamma)\n",
    "        model.fit(training_features, training_target)\n",
    "\n",
    "        #Comparing prediction with testing values\n",
    "        prediction = model.predict(testing_features)\n",
    "\n",
    "        #Get means and std\n",
    "        MSE = mean_squared_error(prediction, testing_target)\n",
    "        MSE_svc += MSE\n",
    "        VarMSE_svc += MSE*MSE\n",
    "        missclass.append(test_size*mean_squared_error(prediction, testing_target))\n",
    "\n",
    "    MSE_svc /= M\n",
    "    VarMSE_svc -= M*MSE_svc**2\n",
    "    VarMSE_svc/=(M-1)\n",
    "    count_svc = test_size*MSE_svc\n",
    "    countVar_svc = test_size*VarMSE_svc\n",
    "    \n",
    "           \n",
    "    return MSE, VarMSE, missclass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel = 'rbf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_vector = [0.2, 0.4, 0.8, 1, 2, 5, 10]\n",
    "gamma_vector = [0.0001, 0.001, 0.01, 0.1]\n",
    "\n",
    "# kernel = 'rbf'\n",
    "opt_rbf = []\n",
    "n_simul = len(C_vector)*len(gamma_vector)\n",
    "count = 0\n",
    "for i in range(len(C_vector)):\n",
    "    C = C_vector[i]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        count += 1\n",
    "        if count % 10 == 0:\n",
    "            print('# simulations done ' + str(count) + '/' + str(n_simul))\n",
    "        gamma = gamma_vector[j]\n",
    "        MSE, VarMSE, missclass = mySVC(C=C, kernel = 'rbf', gamma=gamma, degree=0) # changed mySVC\n",
    "        if test_size*MSE/test_size < 0.1 and test_size*VarMSE/test_size < 0.05:\n",
    "            opt_rbf.append([i,j])\n",
    "\n",
    "print()\n",
    "if len(opt_rbf) == 0: print('no condidates for optimal distributions')\n",
    "else:        \n",
    "    print('candidate indices for optimal distributions')\n",
    "    print(opt_rbf)           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_vector = []\n",
    "for i in range(len(opt_rbf)):\n",
    "    C = C_vector[opt_rbf[i][0]]\n",
    "    gamma = gamma_vector[opt_rbf[i][1]]\n",
    "    MSE, VarMSE, missclass = mySVC(C, kernel = 'rbf', gamma=gamma, degree=0)\n",
    "    axes_vector.append(missclass)\n",
    "    print(r'Average # misclassified samples from ' + str(test_size) + r' samples: ' + \n",
    "              str(test_size*MSE) + r' +- ' + str(test_size*VarMSE) + r' with gamma = ' + str(gamma) + \n",
    "         r', degree ' + str(degree) + ' and C = ' + str(C))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel = 'sigmoid'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_vector = [0.2, 0.4, 0.8, 1, 2, 5, 10]\n",
    "gamma_vector = [0.00001, 0.0001, 0.001]\n",
    "degree_vector = [0,1,2,3,4]\n",
    "\n",
    "# counting how many simulations to be done\n",
    "n_simul_sigm = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        for i in range(len(nu_vector)):\n",
    "            n_simul_sigm += 1\n",
    "\n",
    "print('number of simulations to be done ' + str(n_simul_sigm))\n",
    "\n",
    "\n",
    "# kernel = 'sigmoid'\n",
    "opt_sigm = []\n",
    "count = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        for i in range(len(nu_vector)):\n",
    "            count +=1\n",
    "            if count % 20 == 0:\n",
    "                print('# simulations done ' + str(count) + '/' + str(n_simul_sigm))\n",
    "            nu = nu_vector[i]\n",
    "            MSE, VarMSE, missclass = mySVC(C=C, kernel = 'sigmoid', gamma=gamma, degree=degree)\n",
    "            if test_size*MSE/test_size < 0.15 and test_size*VarMSE/test_size < 0.05:\n",
    "                opt_sigm.append([i,j,k])\n",
    "\n",
    "print()\n",
    "if len(opt_sigm == 0: print('no condidates for optimal distributions')\n",
    "else:        \n",
    "    print('candidate indices for optimal distributions')\n",
    "    print(opt_sigm) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### kernel = 'poly'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C_vector = [0.2, 0.4, 0.8, 1, 2, 5, 10]\n",
    "gamma_vector = [0.00001, 0.0001, 0.001]\n",
    "degree_vector = [0,1,2,3,4]\n",
    "\n",
    "# counting how many simulations to be done\n",
    "n_simul_poly = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        if degree >= 2 and gamma >= 0.01: continue\n",
    "        if degree >= 3 and gamma >= 0.001: continue\n",
    "        for i in range(len(nu_vector)):\n",
    "            n_simul_poly += 1\n",
    "\n",
    "print('number of simulations to be done ' + str(n_simul_poly))\n",
    "\n",
    "\n",
    "# kernel = 'poly'\n",
    "opt_poly = []\n",
    "count = 0\n",
    "for k in range(len(degree_vector)):\n",
    "    degree = degree_vector[k]\n",
    "    for j in range(len(gamma_vector)):\n",
    "        gamma = gamma_vector[j]\n",
    "        if degree >= 2 and gamma >= 0.01: continue\n",
    "        if degree >= 3 and gamma >= 0.001: continue\n",
    "        for i in range(len(nu_vector)):\n",
    "            count +=1\n",
    "            if count % 20 == 0:\n",
    "                print('# simulations done ' + str(count) + '/' + str(n_simul_poly))\n",
    "            nu = nu_vector[i]\n",
    "            MSE, VarMSE, missclass = mySVC(C=C, kernel = 'poly', gamma=gamma, degree=degree)\n",
    "            if test_size*MSE/test_size < 0.15 and test_size*VarMSE/test_size < 0.05:\n",
    "                opt_poly.append([i,j,k])\n",
    "\n",
    "\n",
    "print()              \n",
    "if len(opt_rbf) == 0: print('no condidates for optimal distributions')\n",
    "else:        \n",
    "    print('candidate indices for optimal distributions')\n",
    "    print(opt_rbf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Simulate once again the candidates for optimum to get the histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "axes_vector = []\n",
    "for i in range(len(opt_poly)):\n",
    "    nu = nu_vector[opt_poly[i][0]]\n",
    "    gamma = gamma_vector[opt_poly[i][1]]\n",
    "    degree = degree_vector[opt_poly[i][2]]\n",
    "    MSE, VarMSE, missclass = mySVC(C=C, kernel = 'poly', gamma=gamma, degree=degree)\n",
    "    axes_vector.append(missclass)\n",
    "    print(r'Average # misclassified samples from ' + str(test_size) + r' samples: ' + \n",
    "              str(test_size*MSE) + r' +- ' + str(test_size*VarMSE) + r' with gamma = ' + str(gamma) + \n",
    "         r', degree ' + str(degree) + ' and C = ' + str(C))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup pandas options\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.5f}'.format\n",
    "\n",
    "# Setup numpy options\n",
    "np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "# Get the data\n",
    "# PT_data = pd.read_excel(\"PTResults_trimmed.xlsx\")\n",
    "# PT_data = pd.read_excel(\"PTResults.xlsx\")\n",
    "# PT_data = pd.read_excel(\"Data_Colleff_Entire.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
