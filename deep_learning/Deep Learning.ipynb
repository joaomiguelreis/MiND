{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning for small dataset and one ice location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "\n",
    "\n",
    "class TwoLayerNet(torch.nn.Module):\n",
    "    def __init__(self, D_in, H, D_out):\n",
    "        \"\"\"\n",
    "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
    "        member variables.\n",
    "        \"\"\"\n",
    "        super(TwoLayerNet, self).__init__()\n",
    "        self.linear1 = torch.nn.Linear(D_in, H)\n",
    "        self.linear2 = torch.nn.Linear(H, 10)\n",
    "        self.linear3 = torch.nn.Linear(10, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        In the forward function we accept a Tensor of input data and we must return\n",
    "        a Tensor of output data. We can use Modules defined in the constructor as\n",
    "        well as arbitrary operators on Tensors.\n",
    "        \"\"\"\n",
    "        h_relu = self.linear1(x).clamp(min=0)\n",
    "        h2_relu = self.linear2(h_relu).sigmoid()\n",
    "        y_pred = self.linear3(h2_relu)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# N is size of the training set; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H= 900, 5, 10\n",
    "\n",
    "# Data\n",
    "PT_data = pd.read_excel(\"../PTResults-1000.xlsx\")\n",
    "PT_tensor = torch.tensor(PT_data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([900, 5]) torch.Size([100, 5])\n",
      "torch.Size([900]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "#Eventually change to batch training and shuffle what is training\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "x_train, y_train = torch.utils.data.random_split(PT_tensor, [N, 1000-N])\n",
    "x_train = PT_tensor[:N,1:6].float()\n",
    "y_train = PT_tensor[:N,65].long()#.view(N,1)\n",
    "\n",
    "x_test = PT_tensor[N:,1:6].float()\n",
    "y_test = PT_tensor[N:,65].long()#.view(N,1)\n",
    "\n",
    "D_out = 2#len(y_train[0,:])\n",
    "print(2)\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_init):# Construct our model by instantiating the class defined above\n",
    "    model = model_init\n",
    "\n",
    "    # Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "    # in the SGD constructor will contain the learnable parameters of the two\n",
    "    # nn.Linear modules which are members of the model.\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    for t in range(100):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # Compute and print loss\n",
    "        #print(y_pred.shape, y_train.shape)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        #print(t, loss.item())\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        if t == 1:\n",
    "            optimizer.param_groups[0]['lr'] = 0.01\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test accuracy \n",
    "    \n",
    "    #print(y_pred, y_train)\n",
    "  \n",
    "    #print(y_pred, y_train)\n",
    "    correct_train = ((torch.argmax(y_pred,dim=-1)==y_train)).sum()\n",
    "    acc_train = correct_train.float()/y_train.shape[0]\n",
    "\n",
    "    y_hat = model(x_test)\n",
    "    correct_test = (torch.argmax(y_hat,dim=-1)==y_test).sum()\n",
    "    acc_test = correct_test.float()/y_test.shape[0]\n",
    "\n",
    "    \n",
    "    print('Training accuracy: ', acc_train.item())\n",
    "    print('Testing accuracy: ', acc_test.item())\n",
    "    return model,  acc_train.item(),acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test number 0]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8999999761581421\n",
      "[Test number 1]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 2]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 3]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 4]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 5]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 6]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 7]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 8]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 9]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 10]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 11]\n",
      "Training accuracy:  0.9011111259460449\n",
      "Testing accuracy:  0.8100000023841858\n",
      "[Test number 12]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 13]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 14]\n",
      "Training accuracy:  0.9066666960716248\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 15]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 16]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 17]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 18]\n",
      "Training accuracy:  0.9066666960716248\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 19]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 20]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 21]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 22]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 23]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 24]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 25]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 26]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 27]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 28]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 29]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 30]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 31]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.9200000166893005\n",
      "[Test number 32]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 33]\n",
      "Training accuracy:  0.9066666960716248\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 34]\n",
      "Training accuracy:  0.9066666960716248\n",
      "Testing accuracy:  0.8799999952316284\n",
      "[Test number 35]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 36]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 37]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 38]\n",
      "Training accuracy:  0.9077777862548828\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 39]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 40]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 41]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 42]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.8999999761581421\n",
      "[Test number 43]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 44]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 45]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 46]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 47]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 48]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 49]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 50]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.8600000143051147\n",
      "[Test number 51]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 52]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 53]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 54]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 55]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 56]\n",
      "Training accuracy:  0.9077777862548828\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 57]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 58]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 59]\n",
      "Training accuracy:  0.9066666960716248\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 60]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 61]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 62]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8999999761581421\n",
      "[Test number 63]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 64]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 65]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 66]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 67]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.8999999761581421\n",
      "[Test number 68]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 69]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 70]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 71]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 72]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 73]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 74]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 75]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 76]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 77]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8600000143051147\n",
      "[Test number 78]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 79]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 80]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 81]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 82]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8999999761581421\n",
      "[Test number 83]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 84]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.9200000166893005\n",
      "[Test number 85]\n",
      "Training accuracy:  0.9066666960716248\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 86]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 87]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 88]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 89]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 90]\n",
      "Training accuracy:  0.9011111259460449\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 91]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 92]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 93]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 94]\n",
      "Training accuracy:  0.8899999856948853\n",
      "Testing accuracy:  0.8399999737739563\n",
      "[Test number 95]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 96]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 97]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 98]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.8999999761581421\n",
      "[Test number 99]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 100]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 101]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 102]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 103]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.8799999952316284\n",
      "[Test number 104]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8999999761581421\n",
      "[Test number 105]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.9200000166893005\n",
      "[Test number 106]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 107]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 108]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.8799999952316284\n",
      "[Test number 109]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 110]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 111]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 112]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 113]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 114]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 115]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 116]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 117]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 118]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 119]\n",
      "Training accuracy:  0.9077777862548828\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 120]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 121]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 122]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 123]\n",
      "Training accuracy:  0.9066666960716248\n",
      "Testing accuracy:  0.8999999761581421\n",
      "[Test number 124]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 125]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 126]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 127]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8999999761581421\n",
      "[Test number 128]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 129]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 130]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 131]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 132]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 133]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 134]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 135]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 136]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 137]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 138]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 139]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 140]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 141]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 142]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.9200000166893005\n",
      "[Test number 143]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.9200000166893005\n",
      "[Test number 144]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 145]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 146]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 147]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 148]\n",
      "Training accuracy:  0.8888888955116272\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 149]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.9200000166893005\n",
      "[Test number 150]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 151]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 152]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 153]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 154]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 155]\n",
      "Training accuracy:  0.9077777862548828\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 156]\n",
      "Training accuracy:  0.8766666650772095\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 157]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.8999999761581421\n",
      "[Test number 158]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.9200000166893005\n",
      "[Test number 159]\n",
      "Training accuracy:  0.8999999761581421\n",
      "Testing accuracy:  0.8600000143051147\n",
      "[Test number 160]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 161]\n",
      "Training accuracy:  0.8866666555404663\n",
      "Testing accuracy:  0.8799999952316284\n",
      "[Test number 162]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 163]\n",
      "Training accuracy:  0.898888885974884\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 164]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.8999999761581421\n",
      "[Test number 165]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 166]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8999999761581421\n",
      "[Test number 167]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 168]\n",
      "Training accuracy:  0.897777795791626\n",
      "Testing accuracy:  0.8799999952316284\n",
      "[Test number 169]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 170]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.9200000166893005\n",
      "[Test number 171]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 172]\n",
      "Training accuracy:  0.898888885974884\n",
      "Testing accuracy:  0.8299999833106995\n",
      "[Test number 173]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 174]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 175]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 176]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 178]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 179]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6700000166893005\n",
      "[Test number 180]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 181]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 182]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 183]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 184]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.9200000166893005\n",
      "[Test number 185]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 186]\n",
      "Training accuracy:  0.903333306312561\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 187]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 188]\n",
      "Training accuracy:  0.8999999761581421\n",
      "Testing accuracy:  0.8399999737739563\n",
      "[Test number 189]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 190]\n",
      "Training accuracy:  0.8999999761581421\n",
      "Testing accuracy:  0.8600000143051147\n",
      "[Test number 191]\n",
      "Training accuracy:  0.9066666960716248\n",
      "Testing accuracy:  0.9100000262260437\n",
      "[Test number 192]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 193]\n",
      "Training accuracy:  0.9044444561004639\n",
      "Testing accuracy:  0.8999999761581421\n",
      "[Test number 194]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 195]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 196]\n",
      "Training accuracy:  0.6977777481079102\n",
      "Testing accuracy:  0.6299999952316284\n",
      "[Test number 197]\n",
      "Training accuracy:  0.9066666960716248\n",
      "Testing accuracy:  0.8899999856948853\n",
      "[Test number 198]\n",
      "Training accuracy:  0.902222216129303\n",
      "Testing accuracy:  0.8700000047683716\n",
      "[Test number 199]\n",
      "Training accuracy:  0.9055555462837219\n",
      "Testing accuracy:  0.9100000262260437\n"
     ]
    }
   ],
   "source": [
    "# Run 1000 iterations of the training to get an average\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "for i in range(200):\n",
    "    print('[Test number %i]' %(i))\n",
    "    model_init = TwoLayerNet(D_in, H, D_out)\n",
    "    model, acc_train, acc_test = train(model_init)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([96.,  1.,  0.,  0.,  0.,  0.,  0.,  1.,  3., 21., 36., 42.]),\n",
       " array([0.63      , 0.65416666, 0.67833333, 0.7025    , 0.72666667,\n",
       "        0.75083334, 0.77500001, 0.79916667, 0.82333334, 0.84750001,\n",
       "        0.87166668, 0.89583335, 0.92000002]),\n",
       " <a list of 12 Patch objects>)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(accs_test, bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now with non-zero one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# N is size of the training set; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H= 900, 5, 10\n",
    "\n",
    "# Data\n",
    "PT_data = pd.read_excel(\"../Data_Colleff_Entire (1).xlsx\")\n",
    "PT_tensor = torch.tensor(PT_data.values)\n",
    "\n",
    "#Eventually change to batch training and shuffle what is training\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "x_train, y_train = torch.utils.data.random_split(PT_tensor, [N, 1000-N])\n",
    "x_train = PT_tensor[:N,1:6].float()\n",
    "y_train = PT_tensor[:N,65].float()#.view(N,1)\n",
    "\n",
    "x_test = PT_tensor[N:,1:6].float()\n",
    "y_test = PT_tensor[N:,65].float()#.view(N,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([900, 5]) torch.Size([100, 5])\n",
      "torch.Size([900]) torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "D_out = 1#len(y_train[0,:])\n",
    "print(2)\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_init):# Construct our model by instantiating the class defined above\n",
    "    model = model_init\n",
    "\n",
    "    # Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "    # in the SGD constructor will contain the learnable parameters of the two\n",
    "    # nn.Linear modules which are members of the model.\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-12)\n",
    "    for t in range(100):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # Compute and print loss\n",
    "        #print(y_pred.shape, y_train.shape)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        #print(t, loss.item())\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        if t == 1:\n",
    "            optimizer.param_groups[0]['lr'] = 0.01\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test accuracy \n",
    "    \n",
    "    #print(y_pred, y_train)\n",
    "  \n",
    "    #print(y_pred, y_train)\n",
    "    acc_train = criterion(y_pred,y_train)\n",
    "\n",
    "    y_hat = model(x_test)\n",
    "    acc_test = criterion(y_hat,y_test)\n",
    "\n",
    "    \n",
    "    print('Training loss: ', acc_train.item())\n",
    "    print('Testing loss: ', acc_test.item())\n",
    "    return model,  acc_train.item(),acc_test.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test number 0]\n",
      "Training loss:  0.008945977315306664\n",
      "Testing loss:  0.008765904232859612\n",
      "[Test number 1]\n",
      "Training loss:  0.008884815499186516\n",
      "Testing loss:  0.008683854714035988\n",
      "[Test number 2]\n",
      "Training loss:  0.008896964602172375\n",
      "Testing loss:  0.00869775004684925\n",
      "[Test number 3]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683412335813046\n",
      "[Test number 4]\n",
      "Training loss:  0.00888502225279808\n",
      "Testing loss:  0.008683934807777405\n",
      "[Test number 5]\n",
      "Training loss:  0.008886461146175861\n",
      "Testing loss:  0.008686529472470284\n",
      "[Test number 6]\n",
      "Training loss:  0.009460845030844212\n",
      "Testing loss:  0.009289240464568138\n",
      "[Test number 7]\n",
      "Training loss:  0.008935513906180859\n",
      "Testing loss:  0.008694657124578953\n",
      "[Test number 8]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.00868341512978077\n",
      "[Test number 9]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683279156684875\n",
      "[Test number 10]\n",
      "Training loss:  0.008945099078118801\n",
      "Testing loss:  0.008723118342459202\n",
      "[Test number 11]\n",
      "Training loss:  0.008885846473276615\n",
      "Testing loss:  0.008685088716447353\n",
      "[Test number 12]\n",
      "Training loss:  0.008885220624506474\n",
      "Testing loss:  0.008683142252266407\n",
      "[Test number 13]\n",
      "Training loss:  0.008884813636541367\n",
      "Testing loss:  0.008683446794748306\n",
      "[Test number 14]\n",
      "Training loss:  0.008959242142736912\n",
      "Testing loss:  0.008740101009607315\n",
      "[Test number 15]\n",
      "Training loss:  0.008885796181857586\n",
      "Testing loss:  0.008683586493134499\n",
      "[Test number 16]\n",
      "Training loss:  0.008886430412530899\n",
      "Testing loss:  0.00868607871234417\n",
      "[Test number 17]\n",
      "Training loss:  0.008884851820766926\n",
      "Testing loss:  0.008683490566909313\n",
      "[Test number 18]\n",
      "Training loss:  0.00888488907366991\n",
      "Testing loss:  0.008683456107974052\n",
      "[Test number 19]\n",
      "Training loss:  0.008900586515665054\n",
      "Testing loss:  0.008692077361047268\n",
      "[Test number 20]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683435618877411\n",
      "[Test number 21]\n",
      "Training loss:  0.008884823881089687\n",
      "Testing loss:  0.008683282881975174\n",
      "[Test number 22]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.00868341512978077\n",
      "[Test number 23]\n",
      "Training loss:  0.009009654633700848\n",
      "Testing loss:  0.008768425323069096\n",
      "[Test number 24]\n",
      "Training loss:  0.008899574168026447\n",
      "Testing loss:  0.008696375414729118\n",
      "[Test number 25]\n",
      "Training loss:  0.008885134942829609\n",
      "Testing loss:  0.008683481253683567\n",
      "[Test number 26]\n",
      "Training loss:  0.00888484064489603\n",
      "Testing loss:  0.008683603256940842\n",
      "[Test number 27]\n",
      "Training loss:  0.008886672556400299\n",
      "Testing loss:  0.008685968816280365\n",
      "[Test number 28]\n",
      "Training loss:  0.0088849738240242\n",
      "Testing loss:  0.008684052154421806\n",
      "[Test number 29]\n",
      "Training loss:  0.00888482853770256\n",
      "Testing loss:  0.008683460764586926\n",
      "[Test number 30]\n",
      "Training loss:  0.00894433818757534\n",
      "Testing loss:  0.00874659139662981\n",
      "[Test number 31]\n",
      "Training loss:  0.00892615970224142\n",
      "Testing loss:  0.008727245964109898\n",
      "[Test number 32]\n",
      "Training loss:  0.008895620703697205\n",
      "Testing loss:  0.008695841766893864\n",
      "[Test number 33]\n",
      "Training loss:  0.00888496171683073\n",
      "Testing loss:  0.008683712221682072\n",
      "[Test number 34]\n",
      "Training loss:  0.008884845301508904\n",
      "Testing loss:  0.008683296851813793\n",
      "[Test number 35]\n",
      "Training loss:  0.008884914219379425\n",
      "Testing loss:  0.008683804422616959\n",
      "[Test number 36]\n",
      "Training loss:  0.009397884830832481\n",
      "Testing loss:  0.009218798018991947\n",
      "[Test number 37]\n",
      "Training loss:  0.008884817361831665\n",
      "Testing loss:  0.008683380670845509\n",
      "[Test number 38]\n",
      "Training loss:  0.008884851820766926\n",
      "Testing loss:  0.008683726191520691\n",
      "[Test number 39]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683531545102596\n",
      "[Test number 40]\n",
      "Training loss:  0.008884844370186329\n",
      "Testing loss:  0.008683480322360992\n",
      "[Test number 41]\n",
      "Training loss:  0.008885360322892666\n",
      "Testing loss:  0.008684160187840462\n",
      "[Test number 42]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683453314006329\n",
      "[Test number 43]\n",
      "Training loss:  0.008884936571121216\n",
      "Testing loss:  0.0086837662383914\n",
      "[Test number 44]\n",
      "Training loss:  0.008885771967470646\n",
      "Testing loss:  0.008684666827321053\n",
      "[Test number 45]\n",
      "Training loss:  0.008950464427471161\n",
      "Testing loss:  0.008742522448301315\n",
      "[Test number 46]\n",
      "Training loss:  0.008884849026799202\n",
      "Testing loss:  0.0086836451664567\n",
      "[Test number 47]\n",
      "Training loss:  0.008885206654667854\n",
      "Testing loss:  0.008684269152581692\n",
      "[Test number 48]\n",
      "Training loss:  0.008884827606379986\n",
      "Testing loss:  0.008683553896844387\n",
      "[Test number 49]\n",
      "Training loss:  0.008887258358299732\n",
      "Testing loss:  0.008685251697897911\n",
      "[Test number 50]\n",
      "Training loss:  0.008885220624506474\n",
      "Testing loss:  0.00868316926062107\n",
      "[Test number 51]\n",
      "Training loss:  0.008884834125638008\n",
      "Testing loss:  0.008683472871780396\n",
      "[Test number 52]\n",
      "Training loss:  0.009099657647311687\n",
      "Testing loss:  0.00889554526656866\n",
      "[Test number 53]\n",
      "Training loss:  0.009247328154742718\n",
      "Testing loss:  0.009021544829010963\n",
      "[Test number 54]\n",
      "Training loss:  0.008885208517313004\n",
      "Testing loss:  0.008684273809194565\n",
      "[Test number 55]\n",
      "Training loss:  0.009065001271665096\n",
      "Testing loss:  0.008809066377580166\n",
      "[Test number 56]\n",
      "Training loss:  0.008884814567863941\n",
      "Testing loss:  0.008683265186846256\n",
      "[Test number 57]\n",
      "Training loss:  0.00888683833181858\n",
      "Testing loss:  0.008666845969855785\n",
      "[Test number 58]\n",
      "Training loss:  0.008884821087121964\n",
      "Testing loss:  0.008683454245328903\n",
      "[Test number 59]\n",
      "Training loss:  0.008885080926120281\n",
      "Testing loss:  0.008683500811457634\n",
      "[Test number 60]\n",
      "Training loss:  0.008885000832378864\n",
      "Testing loss:  0.008683158084750175\n",
      "[Test number 61]\n",
      "Training loss:  0.00888498779386282\n",
      "Testing loss:  0.0086830984801054\n",
      "[Test number 62]\n",
      "Training loss:  0.008884829469025135\n",
      "Testing loss:  0.00868177693337202\n",
      "[Test number 63]\n",
      "Training loss:  0.008884855546057224\n",
      "Testing loss:  0.008685383945703506\n",
      "[Test number 64]\n",
      "Training loss:  0.008884813636541367\n",
      "Testing loss:  0.008683483116328716\n",
      "[Test number 65]\n",
      "Training loss:  0.008896599523723125\n",
      "Testing loss:  0.008690593764185905\n",
      "[Test number 66]\n",
      "Training loss:  0.008884820155799389\n",
      "Testing loss:  0.008683334104716778\n",
      "[Test number 67]\n",
      "Training loss:  0.008885358460247517\n",
      "Testing loss:  0.00867468398064375\n",
      "[Test number 68]\n",
      "Training loss:  0.008886336348950863\n",
      "Testing loss:  0.008684033527970314\n",
      "[Test number 69]\n",
      "Training loss:  0.008889107033610344\n",
      "Testing loss:  0.008687365800142288\n",
      "[Test number 70]\n",
      "Training loss:  0.00899224728345871\n",
      "Testing loss:  0.008819477632641792\n",
      "[Test number 71]\n",
      "Training loss:  0.008885039016604424\n",
      "Testing loss:  0.008682935498654842\n",
      "[Test number 72]\n",
      "Training loss:  0.008888761512935162\n",
      "Testing loss:  0.008683931082487106\n",
      "[Test number 73]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683333173394203\n",
      "[Test number 74]\n",
      "Training loss:  0.008884813636541367\n",
      "Testing loss:  0.008683403953909874\n",
      "[Test number 75]\n",
      "Training loss:  0.008884815499186516\n",
      "Testing loss:  0.00868315901607275\n",
      "[Test number 76]\n",
      "Training loss:  0.00899532251060009\n",
      "Testing loss:  0.008691881783306599\n",
      "[Test number 77]\n",
      "Training loss:  0.008884958922863007\n",
      "Testing loss:  0.008683587424457073\n",
      "[Test number 78]\n",
      "Training loss:  0.008884906768798828\n",
      "Testing loss:  0.008683422580361366\n",
      "[Test number 79]\n",
      "Training loss:  0.008908260613679886\n",
      "Testing loss:  0.008706995286047459\n",
      "[Test number 80]\n",
      "Training loss:  0.008884813636541367\n",
      "Testing loss:  0.008683409541845322\n",
      "[Test number 81]\n",
      "Training loss:  0.008888371288776398\n",
      "Testing loss:  0.00868839118629694\n",
      "[Test number 82]\n",
      "Training loss:  0.008884932845830917\n",
      "Testing loss:  0.008681516163051128\n",
      "[Test number 83]\n",
      "Training loss:  0.008884831331670284\n",
      "Testing loss:  0.008685072883963585\n",
      "[Test number 84]\n",
      "Training loss:  0.008884858340024948\n",
      "Testing loss:  0.008683166466653347\n",
      "[Test number 85]\n",
      "Training loss:  0.008884813636541367\n",
      "Testing loss:  0.008683369494974613\n",
      "[Test number 86]\n",
      "Training loss:  0.008884890004992485\n",
      "Testing loss:  0.00868317298591137\n",
      "[Test number 87]\n",
      "Training loss:  0.008887619711458683\n",
      "Testing loss:  0.008686980232596397\n",
      "[Test number 88]\n",
      "Training loss:  0.008885909803211689\n",
      "Testing loss:  0.008684294298291206\n",
      "[Test number 89]\n",
      "Training loss:  0.00888510700315237\n",
      "Testing loss:  0.008683137595653534\n",
      "[Test number 90]\n",
      "Training loss:  0.008892502635717392\n",
      "Testing loss:  0.008686321787536144\n",
      "[Test number 91]\n",
      "Training loss:  0.008884855546057224\n",
      "Testing loss:  0.008683332242071629\n",
      "[Test number 92]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683411404490471\n",
      "[Test number 93]\n",
      "Training loss:  0.008884823881089687\n",
      "Testing loss:  0.008684677071869373\n",
      "[Test number 94]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683303371071815\n",
      "[Test number 95]\n",
      "Training loss:  0.008884930983185768\n",
      "Testing loss:  0.008683829568326473\n",
      "[Test number 96]\n",
      "Training loss:  0.008885534480214119\n",
      "Testing loss:  0.00867327582091093\n",
      "[Test number 97]\n",
      "Training loss:  0.009027093648910522\n",
      "Testing loss:  0.008832952938973904\n",
      "[Test number 98]\n",
      "Training loss:  0.00888487696647644\n",
      "Testing loss:  0.008683272637426853\n",
      "[Test number 99]\n",
      "Training loss:  0.008885027840733528\n",
      "Testing loss:  0.00868339091539383\n",
      "[Test number 100]\n",
      "Training loss:  0.008884815499186516\n",
      "Testing loss:  0.008683355525135994\n",
      "[Test number 101]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683416061103344\n",
      "[Test number 102]\n",
      "Training loss:  0.009084849618375301\n",
      "Testing loss:  0.008867675438523293\n",
      "[Test number 103]\n",
      "Training loss:  0.008889778517186642\n",
      "Testing loss:  0.008688005618751049\n",
      "[Test number 104]\n",
      "Training loss:  0.00888572633266449\n",
      "Testing loss:  0.008685043081641197\n",
      "[Test number 105]\n",
      "Training loss:  0.00888881552964449\n",
      "Testing loss:  0.008689281530678272\n",
      "[Test number 106]\n",
      "Training loss:  0.008884986862540245\n",
      "Testing loss:  0.00868348591029644\n",
      "[Test number 107]\n",
      "Training loss:  0.008884829469025135\n",
      "Testing loss:  0.00868347566574812\n",
      "[Test number 108]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683395572006702\n",
      "[Test number 109]\n",
      "Training loss:  0.008884893730282784\n",
      "Testing loss:  0.008683323860168457\n",
      "[Test number 110]\n",
      "Training loss:  0.00888503435999155\n",
      "Testing loss:  0.008684186264872551\n",
      "[Test number 111]\n",
      "Training loss:  0.00894761923700571\n",
      "Testing loss:  0.008731119334697723\n",
      "[Test number 112]\n",
      "Training loss:  0.008884829469025135\n",
      "Testing loss:  0.008681816048920155\n",
      "[Test number 113]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683431893587112\n",
      "[Test number 114]\n",
      "Training loss:  0.008884824812412262\n",
      "Testing loss:  0.00868326984345913\n",
      "[Test number 115]\n",
      "Training loss:  0.008886500261723995\n",
      "Testing loss:  0.008685250766575336\n",
      "[Test number 116]\n",
      "Training loss:  0.008897224441170692\n",
      "Testing loss:  0.008687996305525303\n",
      "[Test number 117]\n",
      "Training loss:  0.008890560828149319\n",
      "Testing loss:  0.00869067944586277\n",
      "[Test number 118]\n",
      "Training loss:  0.008884871378540993\n",
      "Testing loss:  0.008683113381266594\n",
      "[Test number 119]\n",
      "Training loss:  0.008886396884918213\n",
      "Testing loss:  0.008685042150318623\n",
      "[Test number 120]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.00868341326713562\n",
      "[Test number 121]\n",
      "Training loss:  0.008885289542376995\n",
      "Testing loss:  0.008674896322190762\n",
      "[Test number 122]\n",
      "Training loss:  0.008884813636541367\n",
      "Testing loss:  0.008683483116328716\n",
      "[Test number 123]\n",
      "Training loss:  0.008885231800377369\n",
      "Testing loss:  0.008683785796165466\n",
      "[Test number 124]\n",
      "Training loss:  0.0088861845433712\n",
      "Testing loss:  0.008685795590281487\n",
      "[Test number 125]\n",
      "Training loss:  0.008884858340024948\n",
      "Testing loss:  0.00868330430239439\n",
      "[Test number 126]\n",
      "Training loss:  0.008884815499186516\n",
      "Testing loss:  0.008683521300554276\n",
      "[Test number 127]\n",
      "Training loss:  0.008884819224476814\n",
      "Testing loss:  0.008683569729328156\n",
      "[Test number 128]\n",
      "Training loss:  0.008998126722872257\n",
      "Testing loss:  0.008777545765042305\n",
      "[Test number 129]\n",
      "Training loss:  0.008885443210601807\n",
      "Testing loss:  0.008684439584612846\n",
      "[Test number 130]\n",
      "Training loss:  0.00910511240363121\n",
      "Testing loss:  0.008955692872405052\n",
      "[Test number 131]\n",
      "Training loss:  0.008890344761312008\n",
      "Testing loss:  0.008688317611813545\n",
      "[Test number 132]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.00868338905274868\n",
      "[Test number 133]\n",
      "Training loss:  0.008886205032467842\n",
      "Testing loss:  0.008684128522872925\n",
      "[Test number 134]\n",
      "Training loss:  0.008885486051440239\n",
      "Testing loss:  0.008684090338647366\n",
      "[Test number 135]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683414198458195\n",
      "[Test number 136]\n",
      "Training loss:  0.00888481643050909\n",
      "Testing loss:  0.008683349937200546\n",
      "[Test number 137]\n",
      "Training loss:  0.008884928189218044\n",
      "Testing loss:  0.008683566004037857\n",
      "[Test number 138]\n",
      "Training loss:  0.00888499803841114\n",
      "Testing loss:  0.00868320744484663\n",
      "[Test number 139]\n",
      "Training loss:  0.008885277435183525\n",
      "Testing loss:  0.00868286844342947\n",
      "[Test number 140]\n",
      "Training loss:  0.008891362696886063\n",
      "Testing loss:  0.008689599111676216\n",
      "[Test number 141]\n",
      "Training loss:  0.008885088376700878\n",
      "Testing loss:  0.008688167668879032\n",
      "[Test number 142]\n",
      "Training loss:  0.008889545686542988\n",
      "Testing loss:  0.008689375594258308\n",
      "[Test number 143]\n",
      "Training loss:  0.008885055780410767\n",
      "Testing loss:  0.008690002374351025\n",
      "[Test number 144]\n",
      "Training loss:  0.008886340074241161\n",
      "Testing loss:  0.008685234002768993\n",
      "[Test number 145]\n",
      "Training loss:  0.00889254454523325\n",
      "Testing loss:  0.008692885749042034\n",
      "[Test number 146]\n",
      "Training loss:  0.008884813636541367\n",
      "Testing loss:  0.00868326984345913\n",
      "[Test number 147]\n",
      "Training loss:  0.008884876035153866\n",
      "Testing loss:  0.008683733642101288\n",
      "[Test number 148]\n",
      "Training loss:  0.008905729278922081\n",
      "Testing loss:  0.008703996427357197\n",
      "[Test number 149]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683265186846256\n",
      "[Test number 150]\n",
      "Training loss:  0.008925712667405605\n",
      "Testing loss:  0.008719275705516338\n",
      "[Test number 151]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683421649038792\n",
      "[Test number 152]\n",
      "Training loss:  0.008900067768990993\n",
      "Testing loss:  0.008689124137163162\n",
      "[Test number 153]\n",
      "Training loss:  0.008884813636541367\n",
      "Testing loss:  0.008683586493134499\n",
      "[Test number 154]\n",
      "Training loss:  0.008895066566765308\n",
      "Testing loss:  0.008712138049304485\n",
      "[Test number 155]\n",
      "Training loss:  0.008884855546057224\n",
      "Testing loss:  0.008683467283844948\n",
      "[Test number 156]\n",
      "Training loss:  0.008886896073818207\n",
      "Testing loss:  0.008690695278346539\n",
      "[Test number 157]\n",
      "Training loss:  0.008886581286787987\n",
      "Testing loss:  0.008686523884534836\n",
      "[Test number 158]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683428168296814\n",
      "[Test number 159]\n",
      "Training loss:  0.009067246690392494\n",
      "Testing loss:  0.008891027420759201\n",
      "[Test number 160]\n",
      "Training loss:  0.008885905146598816\n",
      "Testing loss:  0.008685308508574963\n",
      "[Test number 161]\n",
      "Training loss:  0.008884841576218605\n",
      "Testing loss:  0.008683336898684502\n",
      "[Test number 162]\n",
      "Training loss:  0.00888538733124733\n",
      "Testing loss:  0.00868375226855278\n",
      "[Test number 163]\n",
      "Training loss:  0.00900690071284771\n",
      "Testing loss:  0.00879736803472042\n",
      "[Test number 164]\n",
      "Training loss:  0.008884981274604797\n",
      "Testing loss:  0.008683768101036549\n",
      "[Test number 165]\n",
      "Training loss:  0.008897566236555576\n",
      "Testing loss:  0.008696136064827442\n",
      "[Test number 166]\n",
      "Training loss:  0.008892130106687546\n",
      "Testing loss:  0.008690615184605122\n",
      "[Test number 167]\n",
      "Training loss:  0.008884819224476814\n",
      "Testing loss:  0.008684271015226841\n",
      "[Test number 168]\n",
      "Training loss:  0.008887791074812412\n",
      "Testing loss:  0.008683274500072002\n",
      "[Test number 169]\n",
      "Training loss:  0.00888553261756897\n",
      "Testing loss:  0.008684258908033371\n",
      "[Test number 170]\n",
      "Training loss:  0.008885703980922699\n",
      "Testing loss:  0.008683563210070133\n",
      "[Test number 171]\n",
      "Training loss:  0.008903637528419495\n",
      "Testing loss:  0.008702842518687248\n",
      "[Test number 172]\n",
      "Training loss:  0.008885079063475132\n",
      "Testing loss:  0.0086834616959095\n",
      "[Test number 173]\n",
      "Training loss:  0.008896290324628353\n",
      "Testing loss:  0.008698621764779091\n",
      "[Test number 174]\n",
      "Training loss:  0.008884815499186516\n",
      "Testing loss:  0.008683408610522747\n",
      "[Test number 175]\n",
      "Training loss:  0.00900003220885992\n",
      "Testing loss:  0.00878609623759985\n",
      "[Test number 176]\n",
      "Training loss:  0.008884815499186516\n",
      "Testing loss:  0.008683355525135994\n",
      "[Test number 177]\n",
      "Training loss:  0.008884914219379425\n",
      "Testing loss:  0.008683052845299244\n",
      "[Test number 178]\n",
      "Training loss:  0.008884990587830544\n",
      "Testing loss:  0.008683481253683567\n",
      "[Test number 179]\n",
      "Training loss:  0.008885488845407963\n",
      "Testing loss:  0.0086829150095582\n",
      "[Test number 180]\n",
      "Training loss:  0.008884998969733715\n",
      "Testing loss:  0.008683184161782265\n",
      "[Test number 181]\n",
      "Training loss:  0.008884829469025135\n",
      "Testing loss:  0.008685055188834667\n",
      "[Test number 182]\n",
      "Training loss:  0.00888799037784338\n",
      "Testing loss:  0.008684624917805195\n",
      "[Test number 183]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training loss:  0.008949700742959976\n",
      "Testing loss:  0.008753609843552113\n",
      "[Test number 184]\n",
      "Training loss:  0.008885159157216549\n",
      "Testing loss:  0.008683836087584496\n",
      "[Test number 185]\n",
      "Training loss:  0.008885029703378677\n",
      "Testing loss:  0.008683795109391212\n",
      "[Test number 186]\n",
      "Training loss:  0.008893102407455444\n",
      "Testing loss:  0.008693192154169083\n",
      "[Test number 187]\n",
      "Training loss:  0.008884884417057037\n",
      "Testing loss:  0.008683642372488976\n",
      "[Test number 188]\n",
      "Training loss:  0.008884849958121777\n",
      "Testing loss:  0.008683556690812111\n",
      "[Test number 189]\n",
      "Training loss:  0.008885001763701439\n",
      "Testing loss:  0.00868298951536417\n",
      "[Test number 190]\n",
      "Training loss:  0.008885164745151997\n",
      "Testing loss:  0.008683031424880028\n",
      "[Test number 191]\n",
      "Training loss:  0.008884993381798267\n",
      "Testing loss:  0.008683331310749054\n",
      "[Test number 192]\n",
      "Training loss:  0.008892755024135113\n",
      "Testing loss:  0.008694236166775227\n",
      "[Test number 193]\n",
      "Training loss:  0.008890705183148384\n",
      "Testing loss:  0.008687903173267841\n",
      "[Test number 194]\n",
      "Training loss:  0.008884833194315434\n",
      "Testing loss:  0.0086834616959095\n",
      "[Test number 195]\n",
      "Training loss:  0.008884817361831665\n",
      "Testing loss:  0.00868331827223301\n",
      "[Test number 196]\n",
      "Training loss:  0.008892906829714775\n",
      "Testing loss:  0.00868956744670868\n",
      "[Test number 197]\n",
      "Training loss:  0.008884826675057411\n",
      "Testing loss:  0.008683376014232635\n",
      "[Test number 198]\n",
      "Training loss:  0.008884860202670097\n",
      "Testing loss:  0.008683658204972744\n",
      "[Test number 199]\n",
      "Training loss:  0.008884812705218792\n",
      "Testing loss:  0.008683418855071068\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Run 1000 iterations of the training to get an average\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "for i in range(200):\n",
    "    print('[Test number %i]' %(i))\n",
    "    model_init = TwoLayerNet(D_in, H, D_out)\n",
    "    model, acc_train, acc_test = train(model_init)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([177.,  10.,   5.,   2.,   2.,   1.,   1.,   0.,   0.,   0.,   1.,\n",
       "          1.]),\n",
       " array([0.00866685, 0.00871871, 0.00877058, 0.00882244, 0.00887431,\n",
       "        0.00892618, 0.00897804, 0.00902991, 0.00908178, 0.00913364,\n",
       "        0.00918551, 0.00923737, 0.00928924]),\n",
       " <a list of 12 Patch objects>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(accs_test, bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([900, 6]) torch.Size([174100, 6])\n",
      "torch.Size([900]) torch.Size([174100])\n"
     ]
    }
   ],
   "source": [
    "## Reorganize the data set\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# N is size of the training set; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H= 900, 6, 10\n",
    "\n",
    "# Data\n",
    "PT_data = pd.read_excel(\"../PTResults-1000.xlsx\")\n",
    "ices = np.zeros(175*1000)\n",
    "for i in range(len(PT_data)):\n",
    "    ice = PT_data.loc[i].iloc[7:len(PT_data.columns)]\n",
    "    ices[i*175:(i+1)*175] = ice.values\n",
    "\n",
    "PT_data = PT_data.iloc[np.repeat(np.arange(len(PT_data)), 175)]\n",
    "drop_icol = list(range(6,len(PT_data.columns)))\n",
    "\n",
    "PT_data = PT_data.drop(PT_data.columns[drop_icol],axis=1)\n",
    "PT_data['loc'] = np.array(list(range(175))*1000)\n",
    "PT_data['ice'] = ices\n",
    "#print(PT_data)\n",
    "PT_tensor = torch.tensor(PT_data.values)\n",
    "\n",
    "#Eventually change to batch training and shuffle what is training\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "#x_train, y_train = torch.utils.data.random_split(PT_tensor, [N, 1000-N])\n",
    "x_train = PT_tensor[:N,1:-1].float()\n",
    "y_train = PT_tensor[:N,-1].long()#.view(N,1)\n",
    "\n",
    "x_test = PT_tensor[N:,1:-1].float()\n",
    "y_test = PT_tensor[N:,-1].long()#.view(N,1)\n",
    "\n",
    "D_out = 2#len(y_train[0,:])\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_init):# Construct our model by instantiating the class defined above\n",
    "    model = model_init\n",
    "\n",
    "    # Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "    # in the SGD constructor will contain the learnable parameters of the two\n",
    "    # nn.Linear modules which are members of the model.\n",
    "    criterion = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "    for t in range(100):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # Compute and print loss\n",
    "        #print(y_pred.shape, y_train.shape)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        #print(t, loss.item())\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        if t == 1:\n",
    "            optimizer.param_groups[0]['lr'] = 0.01\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test accuracy \n",
    "    \n",
    "    #print(y_pred, y_train)\n",
    "  \n",
    "    #print(y_pred, y_train)\n",
    "    correct_train = ((torch.argmax(y_pred,dim=-1)==y_train)).sum()\n",
    "    acc_train = correct_train.float()/y_train.shape[0]\n",
    "\n",
    "    y_hat = model(x_test)\n",
    "    correct_test = (torch.argmax(y_hat,dim=-1)==y_test).sum()\n",
    "    print(correct_test)\n",
    "    acc_test = correct_test.float()/y_test.shape[0]\n",
    "\n",
    "    print('Training accuracy: ', acc_train.item())\n",
    "    print('Testing accuracy: ', acc_test.item())\n",
    "    return model,  acc_train.item(),acc_test.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test number 0]\n",
      "tensor(160776)\n",
      "Training accuracy:  0.9611111283302307\n",
      "Testing accuracy:  0.9234692454338074\n",
      "[Test number 1]\n",
      "tensor(159991)\n",
      "Training accuracy:  0.9611111283302307\n",
      "Testing accuracy:  0.9189603924751282\n",
      "[Test number 2]\n",
      "tensor(159315)\n",
      "Training accuracy:  0.9599999785423279\n",
      "Testing accuracy:  0.9150775671005249\n",
      "[Test number 3]\n",
      "tensor(130356)\n",
      "Training accuracy:  0.8244444727897644\n",
      "Testing accuracy:  0.7487421035766602\n",
      "[Test number 4]\n",
      "tensor(160586)\n",
      "Training accuracy:  0.9622222185134888\n",
      "Testing accuracy:  0.9223779439926147\n",
      "[Test number 5]\n",
      "tensor(160475)\n",
      "Training accuracy:  0.95333331823349\n",
      "Testing accuracy:  0.9217403531074524\n",
      "[Test number 6]\n",
      "tensor(159183)\n",
      "Training accuracy:  0.9633333086967468\n",
      "Testing accuracy:  0.9143193364143372\n",
      "[Test number 7]\n",
      "tensor(160951)\n",
      "Training accuracy:  0.9599999785423279\n",
      "Testing accuracy:  0.9244744181632996\n",
      "[Test number 8]\n",
      "tensor(158990)\n",
      "Training accuracy:  0.9555555582046509\n",
      "Testing accuracy:  0.9132108092308044\n",
      "[Test number 9]\n",
      "tensor(158777)\n",
      "Training accuracy:  0.9611111283302307\n",
      "Testing accuracy:  0.9119873642921448\n"
     ]
    }
   ],
   "source": [
    "# Run 1000 iterations of the training to get an average\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "for i in range(10):\n",
    "    print('[Test number %i]' %(i))\n",
    "    model_init = TwoLayerNet(D_in, H, D_out)\n",
    "    model, acc_train, acc_test = train(model_init)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 7.]),\n",
       " array([0.74864447, 0.76319597, 0.77774747, 0.79229897, 0.80685047,\n",
       "        0.82140197, 0.83595347, 0.85050497, 0.86505648, 0.87960798,\n",
       "        0.89415948, 0.90871098, 0.92326248]),\n",
       " <a list of 12 Patch objects>)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(accs_test, bins=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-c3cfd4218d72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;31m# Data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mPT_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_excel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Data_Colleff_Entire (1).xlsx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m175\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPT_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPT_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m7\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPT_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "## Reorganize the data set but no zero ones\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "# N is size of the training set; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H= 900, 6, 10\n",
    "\n",
    "# Data\n",
    "PT_data = pd.read_excel(\"../Data_Colleff_Entire (1).xlsx\")\n",
    "ices = np.zeros(175*1000)\n",
    "for i in range(len(PT_data)):\n",
    "    ice = PT_data.loc[i].iloc[7:len(PT_data.columns)]\n",
    "    ices[i*175:(i+1)*175] = ice.values\n",
    "\n",
    "PT_data = PT_data.iloc[np.repeat(np.arange(len(PT_data)), 175)]\n",
    "drop_icol = list(range(6,len(PT_data.columns)))\n",
    "\n",
    "PT_data = PT_data.drop(PT_data.columns[drop_icol],axis=1)\n",
    "PT_data['loc'] = np.array(list(range(175))*1000)\n",
    "PT_data['ice'] = ices\n",
    "#print(PT_data)\n",
    "PT_tensor = torch.tensor(PT_data.values)\n",
    "\n",
    "#Eventually change to batch training and shuffle what is training\n",
    "import torchvision\n",
    "import torch.utils.data\n",
    "#x_train, y_train = torch.utils.data.random_split(PT_tensor, [N, 1000-N])\n",
    "x_train = PT_tensor[:N*175,1:-1].float()\n",
    "y_train = PT_tensor[:N*175,-1].float()#.view(N,1)\n",
    "\n",
    "x_test = PT_tensor[N*175:,1:-1].float()\n",
    "y_test = PT_tensor[N*175:,-1].float()#.view(N,1)\n",
    "\n",
    "D_out = 1#len(y_train[0,:])\n",
    "print(x_train.shape, x_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model_init):# Construct our model by instantiating the class defined above\n",
    "    model = model_init\n",
    "\n",
    "    # Construct our loss function and an Optimizer. The call to model.parameters()\n",
    "    # in the SGD constructor will contain the learnable parameters of the two\n",
    "    # nn.Linear modules which are members of the model.\n",
    "    criterion = torch.nn.MSELoss()\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=1e-12)\n",
    "    for t in range(10):\n",
    "        # Forward pass: Compute predicted y by passing x to the model\n",
    "        y_pred = model(x_train)\n",
    "\n",
    "        # Compute and print loss\n",
    "        #print(y_pred.shape, y_train.shape)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        print(t, loss.item())\n",
    "        # Zero gradients, perform a backward pass, and update the weights.\n",
    "        if t == 1:\n",
    "            optimizer.param_groups[0]['lr'] = 0.01\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Test accuracy \n",
    "    \n",
    "    #print(y_pred, y_train)\n",
    "  \n",
    "    #print(y_pred, y_train)\n",
    "    acc_train = criterion(y_pred,y_train)\n",
    "\n",
    "    y_hat = model(x_test)\n",
    "    acc_test = criterion(y_hat,y_test)\n",
    "\n",
    "    \n",
    "    print('Training loss: ', acc_train.item())\n",
    "    print('Testing loss: ', acc_test.item())\n",
    "    return model,  acc_train.item(),acc_test.item()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Test number 0]\n",
      "0 0.6543065905570984\n"
     ]
    }
   ],
   "source": [
    "# Run 1000 iterations of the training to get an average\n",
    "accs_train = []\n",
    "accs_test = []\n",
    "for i in range(10):\n",
    "    print('[Test number %i]' %(i))\n",
    "    model_init = TwoLayerNet(D_in, H, D_out)\n",
    "    model, acc_train, acc_test = train(model_init)\n",
    "    accs_train.append(acc_train)\n",
    "    accs_test.append(acc_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1.]),\n",
       " array([0.03773526, 0.04399185, 0.05024844, 0.05650503, 0.06276162,\n",
       "        0.06901821, 0.0752748 , 0.08153139, 0.08778797, 0.09404456,\n",
       "        0.10030115, 0.10655774, 0.11281433]),\n",
       " <a list of 12 Patch objects>)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.hist(accs_test, bins=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anabel's plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000e+00, 2.4983e+02, 7.2032e+00,  ..., 5.2107e-01, 0.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 2.4983e+02, 7.2032e+00,  ..., 5.2107e-01, 1.0000e+00,\n",
      "         0.0000e+00],\n",
      "        [0.0000e+00, 2.4983e+02, 7.2032e+00,  ..., 5.2107e-01, 2.0000e+00,\n",
      "         0.0000e+00],\n",
      "        ...,\n",
      "        [9.9900e+02, 2.4444e+02, 5.1776e+00,  ..., 6.0651e-01, 1.7200e+02,\n",
      "         0.0000e+00],\n",
      "        [9.9900e+02, 2.4444e+02, 5.1776e+00,  ..., 6.0651e-01, 1.7300e+02,\n",
      "         0.0000e+00],\n",
      "        [9.9900e+02, 2.4444e+02, 5.1776e+00,  ..., 6.0651e-01, 1.7400e+02,\n",
      "         0.0000e+00]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "print(PT_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175 0.00847867126218693 0.9971741078954962\n"
     ]
    }
   ],
   "source": [
    "abscissa = pd.read_excel(\"../Data_Colleff_Entire (1).xlsx\").columns[7:].tolist()\n",
    "print(len(abscissa), abscissa[0], abscissa[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.6893e+02, 8.5044e-01, 3.1953e-01, 1.6793e-05, 1.1245e+00, 2.5000e+01],\n",
      "        [2.6893e+02, 8.5044e-01, 3.1953e-01, 1.6793e-05, 1.1245e+00, 2.6000e+01],\n",
      "        [2.6893e+02, 8.5044e-01, 3.1953e-01, 1.6793e-05, 1.1245e+00, 2.7000e+01],\n",
      "        ...,\n",
      "        [2.4444e+02, 5.1776e+00, 4.6408e-01, 4.9672e-05, 6.0651e-01, 1.7200e+02],\n",
      "        [2.4444e+02, 5.1776e+00, 4.6408e-01, 4.9672e-05, 6.0651e-01, 1.7300e+02],\n",
      "        [2.4444e+02, 5.1776e+00, 4.6408e-01, 4.9672e-05, 6.0651e-01, 1.7400e+02]])\n",
      "[[2.6893427e+02 8.5044211e-01 3.1952739e-01 1.6793218e-05 1.1244676e+00\n",
      "  2.5000000e+01]\n",
      " [2.6893427e+02 8.5044211e-01 3.1952739e-01 1.6793218e-05 1.1244676e+00\n",
      "  2.6000000e+01]\n",
      " [2.6893427e+02 8.5044211e-01 3.1952739e-01 1.6793218e-05 1.1244676e+00\n",
      "  2.7000000e+01]\n",
      " ...\n",
      " [2.4444402e+02 5.1775527e+00 4.6408129e-01 4.9672046e-05 6.0651124e-01\n",
      "  1.7200000e+02]\n",
      " [2.4444402e+02 5.1775527e+00 4.6408129e-01 4.9672046e-05 6.0651124e-01\n",
      "  1.7300000e+02]\n",
      " [2.4444402e+02 5.1775527e+00 4.6408129e-01 4.9672046e-05 6.0651124e-01\n",
      "  1.7400000e+02]]\n",
      "[[2.3315565e+02 3.9300318e-03 3.0037045e-01 1.0039113e-05 4.0008000e-01\n",
      "  0.0000000e+00]\n",
      " [2.3315565e+02 3.9300318e-03 3.0037045e-01 1.0039113e-05 4.0008000e-01\n",
      "  0.0000000e+00]\n",
      " [2.3315565e+02 3.9300318e-03 3.0037045e-01 1.0039113e-05 4.0008000e-01\n",
      "  0.0000000e+00]\n",
      " ...\n",
      " [2.7314502e+02 9.9788427e+00 7.9920679e-01 4.9901908e-05 1.2238443e+00\n",
      "  1.7400000e+02]\n",
      " [2.7314502e+02 9.9788427e+00 7.9920679e-01 4.9901908e-05 1.2238443e+00\n",
      "  1.7400000e+02]\n",
      " [2.7314502e+02 9.9788427e+00 7.9920679e-01 4.9901908e-05 1.2238443e+00\n",
      "  1.7400000e+02]]\n"
     ]
    }
   ],
   "source": [
    "y_hat = model(x_test)\n",
    "#print(y_hat, y_test)\n",
    "print(x_test)\n",
    "print(x_test.numpy())\n",
    "print(np.sort(x_test.numpy(),axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.6893e+02, 8.5044e-01, 3.1953e-01, 1.6793e-05, 1.1245e+00, 2.5000e+01],\n",
      "        [2.6893e+02, 8.5044e-01, 3.1953e-01, 1.6793e-05, 1.1245e+00, 2.6000e+01],\n",
      "        [2.6893e+02, 8.5044e-01, 3.1953e-01, 1.6793e-05, 1.1245e+00, 2.7000e+01],\n",
      "        ...,\n",
      "        [2.4444e+02, 5.1776e+00, 4.6408e-01, 4.9672e-05, 6.0651e-01, 1.7200e+02],\n",
      "        [2.4444e+02, 5.1776e+00, 4.6408e-01, 4.9672e-05, 6.0651e-01, 1.7300e+02],\n",
      "        [2.4444e+02, 5.1776e+00, 4.6408e-01, 4.9672e-05, 6.0651e-01, 1.7400e+02]])\n"
     ]
    }
   ],
   "source": [
    "## Abscissa specific errors\n",
    "\n",
    "prec_error_3 = np.zeros(175)\n",
    "for i in range(175):\n",
    "    i\n",
    "\n",
    "prec_error_3 = \n",
    "print(x_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################\n",
    "\n",
    "## PLOTTING RESULTS ##\n",
    "\n",
    "plt.rc('font',**{'family':'serif','serif':['Palatino']})\n",
    "plt.rc('text', usetex=True)\n",
    "\n",
    "plt.plot(abscissa, prec_error_3,color='green',label='Tree depth 3')\n",
    "plt.fill_between(abscissa, prec_error_3-prec_errstd_3, prec_error_3+prec_errstd_3,\n",
    "        alpha=0.5,color='#e0e0e0')\n",
    "\n",
    "plt.xlabel(r'\\textbf{Abscissa}',fontsize=12)\n",
    "plt.ylabel(r'\\textbf{Precision} [\\%]',fontsize=16)\n",
    "plt.ylim(-0.5,105)\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
